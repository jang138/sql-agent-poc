"""ìµœì¢… ì‘ë‹µ ìƒì„± ë…¸ë“œ"""

from typing import Literal
from langgraph.types import Command
from langgraph.graph import END

from agents.state import StatsChatbotState
from agents.helpers import get_llm_text
from utils.prompts import RESPONSE_GENERATION_PROMPT


def format_source_section(tables_info: list) -> str:
    """
    ì¶œì²˜ ì •ë³´ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…

    Args:
        tables_info: ì‚¬ìš©ëœ í…Œì´ë¸” ì •ë³´ ë¦¬ìŠ¤íŠ¸

    Returns:
        str: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì¶œì²˜ ì„¹ì…˜
    """
    if not tables_info:
        return ""

    source_lines = ["\n\n---\n**ğŸ“Š ë°ì´í„° ì¶œì²˜**\n"]

    for table in tables_info:
        description = table.get("description", "")
        org_id = table.get("org_id")
        tbl_id = table.get("tbl_id")

        if org_id and tbl_id:
            url = f"https://kosis.kr/statHtml/statHtml.do?orgId={org_id}&tblId={tbl_id}&conn_path=I2"
            source_lines.append(f"- [{description}]({url})")

    return "\n".join(source_lines)


def generate_response(
    state: StatsChatbotState,
) -> Command[Literal["__end__"]]:
    """
    9. ì‘ë‹µ ìƒì„± ë…¸ë“œ (LLM ë‹¨ê³„)

    ìµœì¢… ì‘ë‹µ ìƒì„±
    - ìì—°ì–´ ë‹µë³€
    - ë°ì´í„° (í…Œì´ë¸”)
    - ì¸ì‚¬ì´íŠ¸
    - ì‹œê°í™” ì°¨íŠ¸ (ìˆìœ¼ë©´)
    - ë°ì´í„° ì¶œì²˜ (KOSIS ë§í¬)  # ì¶”ê°€
    """
    try:
        llm = get_llm_text()

        # ì‘ë‹µì— í¬í•¨í•  ë°ì´í„° ê²°ì • (ì•ˆì „í•˜ê²Œ)
        data = state.get("processed_data") or state.get("query_result") or "ë°ì´í„° ì—†ìŒ"

        # ì¸ì‚¬ì´íŠ¸
        insight = state.get("insight", "")

        # ì°¨íŠ¸ ì •ë³´
        chart_info = ""
        if state.get("chart_spec"):
            chart_info = f"ì‹œê°í™”: {state['chart_spec'].get('chart_type', 'none')}"
        else:
            chart_info = "ì‹œê°í™” ì—†ìŒ"

        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        prompt = RESPONSE_GENERATION_PROMPT.format(
            user_query=state.get("user_query", "ì§ˆë¬¸ ì—†ìŒ"),
            data=str(data)[:1000],
            insight=insight,
            chart_info=chart_info,
        )

        # LLM í˜¸ì¶œ
        response = llm.invoke(prompt)
        final_response = response.content.strip()

        # ===== ì¶œì²˜ ì„¹ì…˜ ì¶”ê°€ =====
        source_section = format_source_section(state.get("tables_info", []))
        if source_section:
            final_response += source_section
        # ===== ì¶”ê°€ ë =====

    except Exception as e:
        print(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        print(f"State keys: {list(state.keys())}")
        # Fallback
        data = state.get("processed_data") or state.get("query_result") or "ë°ì´í„° ì—†ìŒ"
        insight = state.get("insight", "")
        final_response = (
            f"ì¡°íšŒ ê²°ê³¼:\n{str(data)}\n\n{insight}"
            if data != "ë°ì´í„° ì—†ìŒ"
            else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        )

        # ===== Fallbackì—ë„ ì¶œì²˜ ì¶”ê°€ =====
        source_section = format_source_section(state.get("tables_info", []))
        if source_section:
            final_response += source_section
        # ===== ì¶”ê°€ ë =====

    return Command(goto=END, update={"final_response": final_response})
